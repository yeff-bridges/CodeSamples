{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating some test data\n",
    "x_in = torch.randn(1, 38)\n",
    "y = torch.rand(1, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Layer Class\n",
    "\n",
    "Here we create a basic layer class to simplify the construction of our more complex networks.\n",
    "\n",
    "We use the superclass `torch.nn.Module` to make generating the list of parameters easier.\n",
    "\n",
    "We use `torch.nn.Parameter()` to define our weights and biases as parameters.\n",
    "\n",
    "We define a `Layer.Forward()` method to simplify our feed-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, size_in, size_out, activation):\n",
    "        super(Layer, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.randn(size_in, size_out, requires_grad=True)\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(\n",
    "            torch.randn(1, size_out, requires_grad=True)\n",
    "        )  \n",
    "        self.activation = activation\n",
    "        \n",
    "    def Forward(self, z_in):\n",
    "        return self.activation(z_in @ self.weights + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize our layer\n",
    "forget = Layer(38, 15, torch.nn.Sigmoid())\n",
    "loss_func = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(forget.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.1202, -0.4046,  1.5454, -1.2349,  0.3550,  1.6598, -0.8704, -1.8320,\n",
      "         -1.3155, -0.3738,  0.2268,  0.4195,  0.2005,  1.1268,  0.3586]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.1212, -0.4036,  1.5444, -1.2339,  0.3559,  1.6588, -0.8694, -1.8310,\n",
      "         -1.3165, -0.3728,  0.2278,  0.4185,  0.2015,  1.1259,  0.3596]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Run a quick test - notice how the bias values change\n",
    "print(forget.bias)\n",
    "out = forget.Forward(x_in)\n",
    "loss = loss_func(out, y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print(forget.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### RNN Class\n",
    "\n",
    "Here we create a class for our simple Recurrent Neural Network (RNN).\n",
    "\n",
    "Once again, we use the superclass `torch.nn.Module` to make generating the list of parameters easier. It will recursively look through and other `torch.nn.Module`'s in the class to get their parameters too.\n",
    "\n",
    "The `RNN.Forward()` method will iterate through a sequence to generate an output after every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, size_in, size_out, size_mem):\n",
    "        super(RNN, self).__init__()\n",
    "        self.size_mem = size_mem\n",
    "        self.mem_layer = Layer(size_in + size_mem, size_mem, torch.tanh)\n",
    "        self.out_layer = Layer(size_mem, size_out, torch.sigmoid)\n",
    "        \n",
    "    def Forward(self, x):\n",
    "        mem = torch.zeros(1, self.size_mem)\n",
    "        for i in range(x.shape[0]):\n",
    "            x_in = x[[i], :] \n",
    "            z_in = torch.cat([x_in, mem], dim = 1)\n",
    "            mem = self.mem_layer.Forward(z_in)\n",
    "            y_hat = self.out_layer.Forward(mem)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize our RNN\n",
    "rnn = RNN(38, 15, 5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4771,  0.9209, -1.9465,  0.6632, -0.0426]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.4781,  0.9209, -1.9475,  0.6638, -0.0436]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Run a quick test - notice how the bias values change\n",
    "print(rnn.mem_layer.bias)\n",
    "y_hat = rnn.Forward(x_in)\n",
    "loss = loss_func(y_hat, y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print()\n",
    "print(rnn.mem_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LSTM Class\n",
    "\n",
    "Write your own Long Short-Term Memory (LSTM) class using the `torch.nn.Module` superclass.\n",
    "\n",
    "Structure your `LSTM.Forward()` function similar to the RNN, so it will iterate through a sequence.\n",
    "\n",
    "<img src=https://i.stack.imgur.com/RHNrZ.jpg width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mildly Atypical\n",
    "class LSTM:\n",
    "    def __init__(self, size_in, size_mem_short, size_mem_long, size_out, out_func):\n",
    "        self.forget_gate = self.Layer(size_in+size_mem_short, size_mem_long, torch.sigmoid)\n",
    "        self.remember_sig = self.Layer(size_in+size_mem_short, size_mem_long, torch.sigmoid)\n",
    "        self.remember_tanh = self.Layer(size_in+size_mem_short, size_mem_long, torch.tanh)\n",
    "        self.recall_short = self.Layer(size_in+size_mem_short, size_mem_short, torch.sigmoid)\n",
    "        self.recall_long = self.Layer(size_mem_long, size_mem_short, torch.tanh)\n",
    "        self.out = self.Layer(size_mem_short, size_out, out_func)\n",
    "        \n",
    "    def Forward(self, x):\n",
    "        mem_short = torch.zeros([1,size_mem_short])\n",
    "        mem_long = torch.zeros([1,size_mem_long])\n",
    "        out = []\n",
    "        \n",
    "        for t in range(x.shape[0]):\n",
    "            z = torch.cat([x,mem_short], dim=1)\n",
    "            mem_long = self.forget_gate(z) * mem_long\n",
    "            mem_long += self.remember_sig(x) * self.remember_tanh(x)\n",
    "            mem_short = self.recall_short(z) * self.recall_long(mem_long)\n",
    "            out.append(self.out(self.mem_short))\n",
    "            \n",
    "        return torch.cat(out, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
